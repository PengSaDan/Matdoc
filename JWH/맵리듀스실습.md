hdfs에 업로드

`hdfs dfs -copyFromLocal /home/hadoop/Downloads/data.csv /user/cctv`

외부 lib추가해서 share → hadoop 파일에 있는 jar 모두 추가

### Mapper

```java
package exam;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class CctvMapper extends Mapper<Object, Text, Text, IntWritable>{
    //one 변수 선언 IntWritable이란 하둡 시스템에서 만들어 놓은 특별한 타입 
		//빅데이터 처리시 직렬화. 역직렬화 시 네트워크에 타고 갈 수 있도록 한다.
		//병렬로 작업했던 것이 직렬화할 수 있도록 한다.
		private final static IntWrsitable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    protected void map(Object key, Text value, Mapper<Object, Text, Text, 
    		IntWritable>.Context context) throws IOException, InterruptedException {

	      // ","으로 잘라서 1번째꺼
        String[] strs = value.toString().split(",");
        word.set(strs[1]);
				//map() 메서드는 출력을 위해 Context의 인스턴스를 제공한다. 
        context.write(word, one);
    }
}
```

### Reducer

```java
package exam;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class CctvReducer extends Reducer<Text, IntWritable, Text, IntWritable>{

	private IntWritable result =new IntWritable();

	@Override
		//중복을 허용하는 key의 형태로 입력받는다.
	protected void reduce(Text key, Iterable<IntWritable> values, 
			Reducer<Text, IntWritable, Text, IntWritable>.Context context ) 
					throws IOException, InterruptedException{
		int sum=0;
		for(IntWritable value : values)
			sum+=value.get();
		result.set(sum);
		context.write(key, result);
	}
}
```

### Main(job)

```java
package exam;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class CctvMain {
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		
		//hdfs를 제어하기 위해서는 Configuration객체를 생성하고 FileSystem의 get메서드로 
		//configureation객체를 전달해 FileSystem을 획득해야 한다. 
		//이때 Configuration은 하줍 환경설정 파일에 접근하기 위한 클래스이다.
		Configuration conf = new Configuration(); //하둡의 구성정보를 가졌다.
		Job job = Job.getInstance(conf, "cctv"); //"cctv"라는 이름의 job생성
		job.setJarByClass(CctvMain.class);
		
		job.setMapperClass(CctvMapper.class);
		job.setCombinerClass(CctvReducer.class);
		job.setReducerClass(CctvReducer.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		FileInputFormat.addInputPath(job, new Path(args[0])); //입력파일 지정
		FileOutputFormat.setOutputPath(job, new Path(args[1])); //목적지 파일 지정
		
		System.exit(job.waitForCompletion(true)? 0 : 1); //맵리듀스 job 제출
	}

}
```

jar파일 생성하기 : 프로젝트 jar파일로 export하기

- 하둡은 최적화된 네트워크 직렬화를 위해 자체적으로 기본 타입 셋을 제공한다.
    - Long → LongWritable
    - String → Text
    - Integer → IntWritable

	---

매퍼의 출력으로 <<관리기관, 목적>, 목적>과 같이 출력합니다. 파티셔너는 복합키 중 관리기관을 기준으로 리듀서를 나누어줍니다. 리듀서는 복합키를 이용하여 관리기관, 목적 순으로 정렬을 수행합니다. 그리고 리듀서에 데이터를 넘기는 그룹핑 기준을 복합키의 관리기관을 이용하여 처리합니다. 리듀서는 입력으로 <<관리기관, 목적>, List<목적>>을 입력으로 받습니다. 여기서 목적의 유니크한 값과 리스트의 전체 개수를 세면 관리기관별 설치목적의 건수와 모든 CCTV의 건수를 알 수 있게 됩니다.

### Mapper

```java

package exam;

import java.io.IOException;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class CctvMapper extends Mapper<Object, Text, CctvComparePair ,Text>{
    private Text one = new Text();

    @Override
    protected void map(Object key, Text value, Mapper<Object, Text, CctvComparePair,Text>
    		.Context context) throws IOException, InterruptedException {

        String[] strs = value.toString().split(",");
        String admin = strs[1];
        String purpose =strs[4];
        
        CctvComparePair pair = new CctvComparePair(admin, purpose);
        one.set(purpose);
        
        context.write(pair, one);
    }
}
```

[하둡 프로그래밍 - Secondary Sorting](https://velog.io/@spdlqjfire/하둡-프로그래밍-Sorting)

1. 기존 키의 값들을 조합한 복합키(Composite Key)를 정의한다. 이 때 키의 값 중에서 어떤 키를 Grouping Key로 사용할지를 결정하게 된다.
2. 복합키의 레코드를 정렬하기 위한 Comparator를 정의한다.
3. Grouping Key를 Paritioning할 파티셔너를 정의한다.
4. Grouping Key를 정렬하기 위한 Comparator를 정의한다.

### ComparePair

```java
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.WritableComparable;

/**
 * 정렬하고자 하는 세컨더리 키를 포함하는 복합 클래스 
 * @author User
 *
 */
public class CctvComparePair implements WritableComparable<CctvComparePair> {

    private String admin; //관리 
    private String purpose; //설치목적

    public CctvComparePair() {

    }

    public CctvComparePair(String admin, String road) {
        super();
        this.admin = admin; 
        this.purpose = road;
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(admin); //UTF형식으로 코딩된 문자열을 출력한다
        out.writeUTF(purpose);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        admin = in.readUTF();
        purpose = in.readUTF();
    }

    @Override
		//복합키와 복합키를 비교해 순서를 정할 때 사용된다.
    public int compareTo(CctvComparePair key) {
        int result = admin.compareTo(key.admin);

        if (result == 0) { //키가 동일한 경우
						//value를 비교하도록 한다.
            result = purpose.compareTo(key.purpose);
        }

        return result;

    }

    @Override
    public String toString() {
        return new StringBuffer().append(admin).append("\t").append(purpose).toString();
    }

    public String getAdmin() {
        return admin;
    }

    public void setAdmin(String admin) {
        this.admin = admin;
    }

    public String getPurpose() {
        return purpose;
    }

    public void setPurpose(String road) {
        this.purpose = road;
    }   
}
```

### Partitioner

Map Task의 출력 데이터를 Reduce Task의 입력 데이터로 보낼지를 결정하고, 이렇게 Partitioning된 데이터는 Map Task의 출력 데이터의 키의 값에 따라 정렬하게 된다.

```java
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * 관리 기관을 기준으로 하는 커스텀 파티셔너
 * 
 * @author User
 *
 */
public class CctvPartitioner extends Partitioner<CctvComparePair, Text> {

    @Override
		//파티셔너는 getPartition 메소드를 호출해 파티셔닝 번호를 조회하게 된다.
    public int getPartition(CctvComparePair key, Text value, int numPartitions) {
        return (key.getAdmin().charAt(0)) % numPartitions;
    }

}
```

### SortComparetor

```java
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * 리듀서의 전체 입력을 정렬하는 기준이 되는 SortComparator 
 * 
 * @author User
 *
 */
public class CctvSortComparator extends WritableComparator {

    public CctvSortComparator() {
        super(CctvComparePair.class, true);
    }

    @Override
    public int compare(WritableComparable a, WritableComparable b) {
        CctvComparePair x = (CctvComparePair) a;
        CctvComparePair y = (CctvComparePair) b;

        return x.compareTo(y);
    }
}
```

### GroupingComparator

리듀서에서 실제 작업을 진행하는 *reduce(KEYIN key, Iterable values, Context context)* 메소드에 values 데이터를 전달하기 위한 그룹핑을 처리하는 클래스를 구현합니다.

관리기관을 기준으로 그룹핑을 처리하여 전달하기 위해 관리기관명으로 데이터를 비교 합니다. 이제 reduce() 메소드에는 관리기관을 기준으로 정렬된 List<설치목적> 값이 전달되게 됩니다.

```java
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * reduce()의 values를 그룹핑하는 커스텀 그룹핑 Comparator 
 * 
 * @author User
 *
 */
public class CctvGroupingComparator extends WritableComparator {

    public CctvGroupingComparator() {
        super(CctvComparePair.class, true);
    }

    @Override
    public int compare(WritableComparable a, WritableComparable b) {
        CctvComparePair x = (CctvComparePair) a;
        CctvComparePair y = (CctvComparePair) b;

        return x.getAdmin().compareTo(y.getAdmin());
    }
}
```

### Reducer

리듀서는 관리주체를 기준으로 정렬된 List<목적> 데이터를 입력으로 받습니다. 이를 이용하여 유니크한 목적의 개수(관리주체별 목적), 전체 목적의 개수(CCTV의 개수)를 세어서 출력하면 됩니다.

```java
import java.io.IOException;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class CctvReducer extends Reducer<CctvComparePair, Text, Text, Text> {

    private Text outputKey = new Text();
    private Text outputValue = new Text();

    @Override
    protected void reduce(CctvComparePair key, Iterable<Text> values, Reducer<CctvComparePair, Text, Text, Text>.Context context) throws IOException, InterruptedException {

        int uniq = 0;
        int sum = 0;

        String previous = "";
        for (Text value : values) {

            String current = value.toString();

            if (!previous.equals(current)) {
                uniq++;
                previous = current;
            }

            sum++;
        }

        outputKey.set(key.getAdmin());
        outputValue.set(new StringBuffer().append(uniq).append("\t").append(sum).toString());

        context.write(outputKey, outputValue);
    }
}
```

### Main(job)

```java

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class CctvMain {

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        Configuration conf = new Configuration();

        Job job = Job.getInstance(conf, "cctv");
        job.setJarByClass(CctvMain.class);

        // 매퍼 리듀서
        job.setMapperClass(CctvMapper.class);
        job.setReducerClass(CctvReducer.class);

        // 파티셔너, 소트, 그룹핑 
        job.setPartitionerClass(CctvPartitioner.class);
        job.setSortComparatorClass(CctvSortComparator.class);
        job.setGroupingComparatorClass(CctvGroupingComparator.class);

        // 맵의 출력 
        job.setMapOutputKeyClass(CctvComparePair.class);
        job.setMapOutputValueClass(Text.class);
        // 리듀서의 출력 
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```